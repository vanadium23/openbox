---
{"dg-publish":true,"up":"[[Courses]]","tags":["courses"],"rating":4,"source":"https://stepik.org/course/524/promo","date":"2023-09-05T11:29:02+03:00","modified_at":"2024-03-26T09:52:20+03:00","dg-path":"/courses/stepik Основы статистики - часть 2.md","permalink":"/courses/stepik-osnovy-statistiki-chast-2/","dgPassFrontmatter":true}
---




Prev:: [[Openbox/courses/stepik Основы статистики\|stepik Основы статистики]]

## Материалы

Анализ номинативных данных
- номинативные данные - это категориальный признак, который как правило, кодируем числами, но над ними можно применять только операцию моды.
- почти все переменные можно увести и в количественную, и в номинативные, и в ранговые. Например, рост можно брать как число, как ранг (число в порядке по росту), и номинативную (0 - низкий, 1 - высокий)
- расстояние Пирсона (хи квадрат) - это сумма разницы между наблюдаемое и ожидаемого значения, нормированное на ожидаемое частоту. Это просто показатель того насколько мы отклонились от наших ожиданий (в процентах?)
- важно, что для p критерия в случае критерия хи квадрат - в учёт берётся количество **независимых** переменных.
- для анализа двух (нескольких?) номинативных переменных между собой необходимо составить ожидаемые значения в случае их независимости и сравнить с теми данными, которые мы получили.
- поправка Йетса - дополнительное уменьшение вероятности для таблички два на два в номинативных переменных (-0.5 в формуле пирсона)
- Количество независимых переменных в таблице сопряжённости = (n-1)\*(m-1). Минимальное количество наблюдений в каждой ячейке - должно быть больше 5.
- Точный критерий Фишера - это поправка для того, чтобы обойти условие для наличия минимум 5 наблюдений в каждой ячейки.

Логистическая регрессия
- В рамках логистической регрессии мы хотим, чтобы наша зависимая переменная была выражена через формулу с коэфициентами вероятностей той или иной переменной.
- Проблема в том, что вероятность принимает значение от нуля до единицы, а нам нужна вся прямая. Для этого вводят термин "шансы" - это отношение вероятности успеха к вероятности неуспеха. Но шансы лежат от 0 до бесконечности.
- Потом применяем логарифм к шансам, чтобы оно лежало в диапазоне от минус бесконечности до плюс бесконечности.
- При построение логистической регресси с номинативной переменной - показатель intercept означает шансы положительного исхода для первого предиектора, а последующие коэффициенты - это отношение шансов между исходным классом и выбранным предиктором.
- При увеличение количества зависимых переменных мы вводим комбинации не только соотношение шансов между классами, но и соотношение между разными номинативными переменными. Так работает только в случае, если переменные между собой имеют взаимосвязь.
- Нормальность выборочных данных для t-критерия не так важна, как симметричность его между двумя группами независимых данных. Иными словами, если обе выборки отклонены более менее одинаково, то и в результате мы получим снова нормальное распределение с серединой в 0 и дисперсией в 1.
- Перед применением t теста, стоит проверить что у нас соблюдается нормальное распределение. Если у нас выборка ассиметричная или ненормальная, то стоит перейти на U критерий Манна - Уитни.
- Если нарушается и критерий гомогенности, и критерий нормальности распределения, то берётся критерий Краскера Уоллиса.

Кластерный анализ
- Научение без учителя означает, что у модели нет обратной связи. То есть модель не может подстроиться под какую-то формулу.
- Одно из решений для кластерного анализа - это метод k-means.
- Метод k-means: выбираем N точек на плоскости, выбираем ближайшие наблюдения к этим точкам, смещаем изначальные точки в центроиды, перекрашиваем точки, смещаем / перекрашиваем, пока не найдём такое положение, что при очередном изменение наблюдения не меняют принадлежность групп.
- Для k-means есть две основные проблемы:
    - как выбрать положение центроидов на плоскости - для этого есть несколько методов выбора наиболее хорошего положения, чем просто рандом. Плюс можно гонять метод k means несколько раз, чтобы получить устойчивую картинку.
    - как выбрать количество кластеров. Один из методов - это считать сумму квадратов расстояний до центроидов, и мы ищем точку в которой идёт резкий перелом, после которого не сильно меняется эта сумма. Найдя такой локальный минимум, мы считаем что количество кластеров равно этому числу +/- 1.
- Кластеризацию можно делать ещё и с помощью группы методов иерархической кластеризации. В иерархической кластерации мы всегда начинаем с предположения о том, что количество кластеров = количеству наблюдений, а потом алгоритм решает как объединять эти кластеры. Результат алгоритма в дендрограмме, которую надо обработать вручную.
- Самый простой способ кластерной иерархии называется метод ближайшего соседа. Он начинает с того, что у нас количество кластеров = количеству наблюдений, после чего мы считаем расстояния между всеми кластерами. Дальше с минимальным расстоянием - мы начинаем объединять. Дальше уже считается расстояние между центроидами и точками.
- Анализ главных компонент - это метод, который пытается найти новые оси, которые поделят наши наблюдения. Суть этого смысла в том, чтобы уменьшить размерность наших наблюдений добавив немножко ошибки на уровне дисперсии. Особенно, это помогает при построение корелляции с мультиколлерильностью.
- График biplot, я не понял.

## Цитатник

- [[Openbox/quotes/202402211004\|202402211004]]: распределение хи-квадрат
- [[Openbox/quotes/202402241120\|202402241120]]: критерий Фишера и чай
- [[Openbox/quotes/202402261000\|202402261000]]: перевод шансов в коэффициент регрессии
- [[Openbox/quotes/202403050932\|202403050932]]: про отношение к теориям
- [[Openbox/quotes/202403060932\|202403060932]]: как выбрать правильный метод анализа

