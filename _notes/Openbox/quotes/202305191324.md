---
{"dg-publish":true,"title":"attention-based networks","tags":["quotes"],"date":"2023-05-19T13:24:34+04:00","modified_at":"2023-07-23T21:43:24+03:00","dg-path":"/quotes/202305191324.md","permalink":"/quotes/202305191324/","dgPassFrontmatter":true}
---


> Recurrent networks are rapidly being replaced by a newer form of network based on the idea of attention [11]. The difference in attention-based networks is that they work on whole sequences rather than one token at a time. They include a processing block known as a transformer that uses attention to provide a mechanism with which the network can learn how each token in the input sequence influences other tokens.

Link:: [[Openbox/books/The Little Learner\|The Little Learner]]
